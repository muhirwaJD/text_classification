{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhirwaJD/text_classification/blob/main/Carine_lstm_ml/lstm_ml_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "id": "view-in-github"
    },
    {
      "cell_type": "markdown",
      "id": "b3e9aed6",
      "metadata": {
        "id": "b3e9aed6"
      },
      "source": [
        "# lstm Machine Learning for BBC News Text Classification\n",
        "\n",
        "## Table of Contents\n",
        "1. [Setup & Imports](#setup)\n",
        "2. [Data Loading & Exploration](#data)\n",
        "3. [Data Preprocessing](#preprocessing)\n",
        "4. [Feature Extraction - TF-IDF](#tfidf)\n",
        "5. [Feature Extraction - GloVe](#glove)\n",
        "6. [Feature Extraction - FastText](#fasttext)\n",
        "7. [Model Training - LSTM](#lstm)\n",
        "8. [Results Comparison](#comparison)\n",
        "9. [Conclusion](#conclusion)\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YVPvo9NFevD0",
      "metadata": {
        "id": "YVPvo9NFevD0"
      },
      "source": [
        "### 1. Setup and import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5d845a38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d845a38",
        "outputId": "f9c892af-6cef-40f5-8d0e-abf71f1be37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip install gensim\n",
        "# Text processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Gensim for FastText\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Joblib for model saving\n",
        "import joblib\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # forces CPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cb66d743",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb66d743",
        "outputId": "3a9f69fd-9cb9-456d-85fb-f173fa1f4862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yer1j31FPDvR",
        "outputId": "2b19da90-7cd6-4a61-85d6-9f30b9cb19e0"
      },
      "id": "yer1j31FPDvR",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85ae025",
      "metadata": {
        "id": "e85ae025"
      },
      "source": [
        "\n",
        "## 2. Data Loading & Exploration <a id='data'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b18ac9d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "b18ac9d3",
        "outputId": "31b45a90-123f-45a4-8df6-dcac7075cb3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading preprocessed datasets...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/text_classification (1)/data/preprocessed_data/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1092338892.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading preprocessed datasets...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/text_classification (1)/data/preprocessed_data/train.csv'"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "BASE_DIR = Path('/content/drive/MyDrive/text_classification (1)/data/preprocessed_data')\n",
        "TRAIN_PATH = BASE_DIR / 'train.csv'\n",
        "VAL_PATH = BASE_DIR / 'validation.csv'\n",
        "TEST_PATH = BASE_DIR / 'test.csv'\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading preprocessed datasets...\")\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "val_df = pd.read_csv(VAL_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "\n",
        "print(f\"\\nDataset Sizes:\")\n",
        "print(f\"Training set: {len(train_df):,} samples\")\n",
        "print(f\"Validation set: {len(val_df):,} samples\")\n",
        "print(f\"Test set: {len(test_df):,} samples\")\n",
        "print(f\"Total: {len(train_df) + len(val_df) + len(test_df):,} samples\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 samples from training set:\")\n",
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903ef206",
      "metadata": {
        "id": "903ef206"
      },
      "outputs": [],
      "source": [
        "# Check columns\n",
        "print(\"Training columns:\", train_df.columns.tolist())\n",
        "print(\"Validation columns:\", val_df.columns.tolist())\n",
        "print(\"Test columns:\", test_df.columns.tolist())\n",
        "\n",
        "# Preview first few rows to see labels\n",
        "print(\"\\nTraining data sample:\")\n",
        "print(train_df.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae177a98",
      "metadata": {
        "id": "ae177a98"
      },
      "outputs": [],
      "source": [
        "# Basic dataset information\n",
        "print(\"Training Data Info:\")\n",
        "print(train_df.info())\n",
        "\n",
        "print(\"\\nColumn Names:\", train_df.columns.tolist())\n",
        "print(\"\\nCategories:\", train_df['category'].unique())\n",
        "print(f\"Number of categories: {train_df['category'].nunique()}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(train_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84fa8522",
      "metadata": {
        "id": "84fa8522"
      },
      "outputs": [],
      "source": [
        "def text_length_analysis(df, name):\n",
        "    df['text_length'] = df['text'].astype(str).apply(len)\n",
        "    df['word_count'] = df['text'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    print(f\"\\n{name} Text Length Statistics:\")\n",
        "    print(df[['text_length', 'word_count']].describe())\n",
        "\n",
        "text_length_analysis(train_df, \"Training\")\n",
        "text_length_analysis(val_df, \"Validation\")\n",
        "text_length_analysis(test_df, \"Test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d159ea2",
      "metadata": {
        "id": "6d159ea2"
      },
      "outputs": [],
      "source": [
        "# Create output directory if it doesn't exist\n",
        "output_dir = '/content/drive/MyDrive/text_classification (1)/results (1)'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Text length boxplot\n",
        "train_df.boxplot(\n",
        "    column='text_length',\n",
        "    by='category',\n",
        "    ax=axes[0],\n",
        "    grid=False\n",
        ")\n",
        "axes[0].set_title('Text Length Distribution by Category', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Category', fontsize=10)\n",
        "axes[0].set_ylabel('Text Length (characters)', fontsize=10)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Word count boxplot\n",
        "train_df.boxplot(\n",
        "    column='word_count',\n",
        "    by='category',\n",
        "    ax=axes[1],\n",
        "    grid=False\n",
        ")\n",
        "axes[1].set_title('Word Count Distribution by Category', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Category', fontsize=10)\n",
        "axes[1].set_ylabel('Word Count', fontsize=10)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.suptitle('')  # removes automatic pandas title\n",
        "plt.tight_layout()\n",
        "plt.savefig(\n",
        "    os.path.join(output_dir, 'text_length_analysis.png'),\n",
        "    dpi=300,\n",
        "    bbox_inches='tight'\n",
        ")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "178e63af",
      "metadata": {
        "id": "178e63af"
      },
      "source": [
        "\n",
        "## 3. Data Preprocessing <a id='preprocessing'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59da72c",
      "metadata": {
        "id": "f59da72c"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text_strong(\n",
        "    text,\n",
        "    remove_stopwords=True,\n",
        "    lemmatize=True,\n",
        "    normalize_numbers=True\n",
        "):\n",
        "    # Ensure string & lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "    # Remove URLs & emails\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "\n",
        "    # Expand contractions\n",
        "    text = expand_contractions(text)\n",
        "\n",
        "    # Normalize numbers\n",
        "    if normalize_numbers:\n",
        "        text = re.sub(r'\\d+', ' num ', text)\n",
        "\n",
        "    # Remove punctuation & special characters\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    if remove_stopwords:\n",
        "        tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    if lemmatize:\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "    # Remove junk tokens\n",
        "    tokens = [\n",
        "        t for t in tokens\n",
        "        if 3 <= len(t) <= 20 and t.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Final clean text\n",
        "    return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5371f262",
      "metadata": {
        "id": "5371f262"
      },
      "outputs": [],
      "source": [
        "# ====== NLP PREPROCESSING SETUP (RUN ONCE) ======\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Contractions\n",
        "CONTRACTIONS = {\n",
        "    \"don't\": \"do not\", \"can't\": \"can not\", \"won't\": \"will not\",\n",
        "    \"i'm\": \"i am\", \"it's\": \"it is\", \"that's\": \"that is\",\n",
        "    \"what's\": \"what is\", \"there's\": \"there is\",\n",
        "    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\", \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\", \"wouldn't\": \"would not\",\n",
        "    \"couldn't\": \"could not\", \"shouldn't\": \"should not\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text):\n",
        "    for c, e in CONTRACTIONS.items():\n",
        "        text = re.sub(rf\"\\b{c}\\b\", e, text)\n",
        "    return text\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(\n",
        "    text,\n",
        "    remove_stopwords=True,\n",
        "    lemmatize=True,\n",
        "    normalize_numbers=True\n",
        "):\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove HTML\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "    # Remove URLs / emails\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "\n",
        "    # Expand contractions\n",
        "    text = expand_contractions(text)\n",
        "\n",
        "    # Normalize numbers\n",
        "    if normalize_numbers:\n",
        "        text = re.sub(r'\\d+', ' num ', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stopword removal\n",
        "    if remove_stopwords:\n",
        "        tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    if lemmatize:\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "    # Remove junk tokens\n",
        "    tokens = [t for t in tokens if 3 <= len(t) <= 20 and t.isalpha()]\n",
        "\n",
        "    return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256e0315",
      "metadata": {
        "id": "256e0315"
      },
      "outputs": [],
      "source": [
        "print(\"Preprocessing training set...\")\n",
        "train_df['processed_text'] = train_df['text'].apply(preprocess_text_strong)\n",
        "\n",
        "print(\"Preprocessing validation set...\")\n",
        "val_df['processed_text'] = val_df['text'].apply(preprocess_text_strong)\n",
        "\n",
        "print(\"Preprocessing test set...\")\n",
        "test_df['processed_text'] = test_df['text'].apply(preprocess_text_strong)\n",
        "\n",
        "print(\"Preprocessing complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963196ab",
      "metadata": {
        "id": "963196ab"
      },
      "outputs": [],
      "source": [
        "# Define features and labels\n",
        "X_train = train_df['processed_text']\n",
        "y_train = train_df['category']\n",
        "\n",
        "X_val   = val_df['processed_text']\n",
        "y_val   = val_df['category']\n",
        "\n",
        "X_test  = test_df['processed_text']\n",
        "y_test  = test_df['category']\n",
        "\n",
        "# Encode labels to integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_val_enc   = label_encoder.transform(y_val)\n",
        "y_test_enc  = label_encoder.transform(y_test)\n",
        "\n",
        "print(\"Number of classes:\", len(label_encoder.classes_))\n",
        "print(\"Sample encoded labels:\", y_train_enc[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d810251b",
      "metadata": {
        "id": "d810251b"
      },
      "source": [
        "## 4. Feature Extraction - TF-IDF <a id='tfidf'></a>\n",
        "\n",
        "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic that reflects how important a word is to a document in a collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a47a3ba",
      "metadata": {
        "id": "7a47a3ba"
      },
      "outputs": [],
      "source": [
        "# ======= Imports =======\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assume these already exist from previous steps:\n",
        "# train_df, val_df, test_df with column 'processed_text'\n",
        "\n",
        "X_train = train_df['processed_text']\n",
        "X_val   = val_df['processed_text']\n",
        "X_test  = test_df['processed_text']\n",
        "\n",
        "\n",
        "\n",
        "# ====== TF-IDF Vectorizer ======\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,  # Keep top 5000 features\n",
        "    min_df=2,           # Ignore terms that appear in less than 2 documents\n",
        "    max_df=0.8,         # Ignore terms that appear in more than 80% of documents\n",
        "    ngram_range=(1, 2)  # Use unigrams and bigrams\n",
        ")\n",
        "\n",
        "# Fit on training data, transform train/val/test\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf   = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf  = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF features created!\")\n",
        "print(f\"Train shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Validation shape: {X_val_tfidf.shape}\")\n",
        "print(f\"Test shape: {X_test_tfidf.shape}\")\n",
        "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
        "\n",
        "# ====== Optional: reshape for LSTM ======\n",
        "# NOTE: TF-IDF is not naturally sequential, but this matches your previous LSTM idea\n",
        "X_train_tfidf_seq = np.expand_dims(X_train_tfidf.toarray(), axis=1)\n",
        "X_val_tfidf_seq   = np.expand_dims(X_val_tfidf.toarray(), axis=1)\n",
        "X_test_tfidf_seq  = np.expand_dims(X_test_tfidf.toarray(), axis=1)\n",
        "\n",
        "print(\"TF-IDF reshaped for LSTM:\", X_train_tfidf_seq.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337928fa",
      "metadata": {
        "id": "337928fa"
      },
      "outputs": [],
      "source": [
        "def get_top_tfidf_terms(vectorizer, X, y, category, n=10):\n",
        "    \"\"\"\n",
        "    Returns the top n TF-IDF terms for a given category.\n",
        "    \"\"\"\n",
        "    # Boolean mask -> row indices\n",
        "    mask = y == category\n",
        "    indices = np.where(mask)[0]  # get row numbers\n",
        "\n",
        "    # Select only rows for this category\n",
        "    X_category = X[indices]\n",
        "\n",
        "    # Sum TF-IDF scores across documents\n",
        "    tfidf_sum = np.array(X_category.sum(axis=0)).flatten()\n",
        "\n",
        "    # Feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Indices of top n terms\n",
        "    top_indices = tfidf_sum.argsort()[-n:][::-1]\n",
        "\n",
        "    return [(feature_names[i], tfidf_sum[i]) for i in top_indices]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3cef92f",
      "metadata": {
        "id": "e3cef92f"
      },
      "source": [
        "\n",
        "## 5. Feature Extraction - GloVe <a id='glove'></a>\n",
        "\n",
        "**GloVe (Global Vectors)** creates word embeddings by aggregating global word-word co-occurrence statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6fd44f",
      "metadata": {
        "id": "4d6fd44f"
      },
      "outputs": [],
      "source": [
        "# ====== Imports ======\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# ====== Path to GloVe file ======\n",
        "GLOVE_PATH = '/content/drive/MyDrive/text_classification (1)/glove/glove.6B.100d.txt'  # update if needed\n",
        "# ====== Load GloVe embeddings ======\n",
        "def load_glove_embeddings(glove_path):\n",
        "    \"\"\"\n",
        "    Loads GloVe embeddings into a dictionary.\n",
        "    Key: word\n",
        "    Value: np.array vector\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "\n",
        "    if not os.path.exists(glove_path):\n",
        "        print(f\"WARNING: GloVe file not found at {glove_path}\")\n",
        "        print(\"Please download from: https://nlp.stanford.edu/projects/glove/\")\n",
        "        print(\"Recommended: glove.6B.zip (glove.6B.100d.txt or 300d)\")\n",
        "        return None\n",
        "\n",
        "    print(\"Loading GloVe embeddings...\")\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "\n",
        "    # Summary\n",
        "    print(f\"Loaded {len(embeddings)} word vectors\")\n",
        "    print(f\"Vector dimension: {len(next(iter(embeddings.values())))}\")\n",
        "    return embeddings\n",
        "\n",
        "# ===== Load GloVe =====\n",
        "glove_embeddings = load_glove_embeddings(GLOVE_PATH)\n",
        "\n",
        "# Optional test\n",
        "if glove_embeddings:\n",
        "    test_word = 'amazon'\n",
        "    if test_word in glove_embeddings:\n",
        "        print(f\"\\nExample embedding for '{test_word}':\\n\", glove_embeddings[test_word][:10], \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c10921",
      "metadata": {
        "id": "35c10921"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "# -------------------------\n",
        "# SETTINGS\n",
        "# -------------------------\n",
        "glove_path = \"/content/drive/MyDrive/text_classification (1)/glove/glove.6B.300d.txt\"  # change this to your local GloVe file path\n",
        "embedding_dim = 300\n",
        "max_len = 100  # for sequence embeddings\n",
        "\n",
        "# -------------------------\n",
        "# LOAD GLOVE\n",
        "# -------------------------\n",
        "print(\"Loading GloVe embeddings...\")\n",
        "glove_embeddings = {}\n",
        "with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        word = parts[0]\n",
        "        vector = np.array(parts[1:], dtype=np.float32)\n",
        "        if vector.shape[0] == embedding_dim:  # sanity check\n",
        "            glove_embeddings[word] = vector\n",
        "print(f\"Loaded {len(glove_embeddings)} word vectors.\")\n",
        "\n",
        "# -------------------------\n",
        "# TOKENIZATION\n",
        "# -------------------------\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return text.lower().split()\n",
        "\n",
        "def get_word_embedding(word: str, glove_embeddings: dict) -> np.ndarray:\n",
        "    return glove_embeddings.get(word, None)\n",
        "\n",
        "# -------------------------\n",
        "# DOCUMENT-LEVEL EMBEDDING\n",
        "# -------------------------\n",
        "def get_doc_embedding_glove(text: str, glove_embeddings: dict, embedding_dim=300) -> np.ndarray:\n",
        "    tokens = tokenize(text)\n",
        "    embeddings = [get_word_embedding(word, glove_embeddings) for word in tokens]\n",
        "    embeddings = [e for e in embeddings if e is not None]\n",
        "\n",
        "    if not embeddings:\n",
        "        return np.zeros(embedding_dim, dtype=np.float32)\n",
        "    return np.mean(np.stack(embeddings, axis=0), axis=0).astype(np.float32)\n",
        "\n",
        "def create_doc_embeddings(texts, glove_embeddings, embedding_dim=300):\n",
        "    return np.vstack([get_doc_embedding_glove(text, glove_embeddings, embedding_dim) for text in texts])\n",
        "\n",
        "# -------------------------\n",
        "# SEQUENCE-LEVEL EMBEDDING (FOR LSTM)\n",
        "# -------------------------\n",
        "def get_sequence_embedding_glove(text: str, glove_embeddings: dict, embedding_dim=300, max_len=100) -> np.ndarray:\n",
        "    tokens = tokenize(text)\n",
        "    embeddings = [get_word_embedding(word, glove_embeddings) for word in tokens]\n",
        "    embeddings = [e for e in embeddings if e is not None]\n",
        "\n",
        "    seq = embeddings[:max_len]  # truncate\n",
        "    pad_length = max_len - len(seq)\n",
        "    if pad_length > 0:\n",
        "        seq += [np.zeros(embedding_dim, dtype=np.float32)] * pad_length\n",
        "\n",
        "    return np.stack(seq, axis=0).astype(np.float32)\n",
        "\n",
        "def create_sequence_embeddings(texts, glove_embeddings, embedding_dim=300, max_len=100):\n",
        "    return np.stack([get_sequence_embedding_glove(text, glove_embeddings, embedding_dim, max_len) for text in texts])\n",
        "\n",
        "# -------------------------\n",
        "# USAGE EXAMPLE\n",
        "# -------------------------\n",
        "# Replace X_train, X_val, X_test with your actual datasets\n",
        "# Example: X_train = [\"This is a sentence.\", \"Another document!\"]\n",
        "\n",
        "print(\"Creating document-level embeddings...\")\n",
        "X_train_doc = create_doc_embeddings(X_train, glove_embeddings, embedding_dim)\n",
        "X_val_doc   = create_doc_embeddings(X_val, glove_embeddings, embedding_dim)\n",
        "X_test_doc  = create_doc_embeddings(X_test, glove_embeddings, embedding_dim)\n",
        "print(\"Document-level shapes:\", X_train_doc.shape, X_val_doc.shape, X_test_doc.shape)\n",
        "\n",
        "print(\"Creating sequence-level embeddings for LSTM...\")\n",
        "X_train_seq = create_sequence_embeddings(X_train, glove_embeddings, embedding_dim, max_len)\n",
        "X_val_seq   = create_sequence_embeddings(X_val, glove_embeddings, embedding_dim, max_len)\n",
        "X_test_seq  = create_sequence_embeddings(X_test, glove_embeddings, embedding_dim, max_len)\n",
        "print(\"Sequence-level shapes:\", X_train_seq.shape, X_val_seq.shape, X_test_seq.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3551902",
      "metadata": {
        "id": "e3551902"
      },
      "source": [
        "\n",
        "## 6. Feature Extraction - FastText <a id='fasttext'></a>\n",
        "\n",
        "**FastText** learns word embeddings using neural networks and subword information, making it robust to rare words and typos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025881b2",
      "metadata": {
        "id": "025881b2"
      },
      "outputs": [],
      "source": [
        "# ===== FastText Training (RAM-safe, single cell) =====\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# -------- Re-iterable streaming corpus (FIX) --------\n",
        "class corpus_iterator:\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "\n",
        "    def __iter__(self):\n",
        "        for doc in self.texts:\n",
        "            yield simple_preprocess(doc)\n",
        "\n",
        "# -------- Hyperparameters --------\n",
        "VECTOR_SIZE = 100\n",
        "WINDOW = 5\n",
        "MIN_COUNT = 5\n",
        "WORKERS = 2\n",
        "SG = 1\n",
        "EPOCHS = 15\n",
        "MIN_N = 3\n",
        "MAX_N = 5\n",
        "ALPHA = 0.025\n",
        "MIN_ALPHA = 0.0001\n",
        "\n",
        "print(\"Initializing FastText model...\")\n",
        "fasttext_model = FastText(\n",
        "    vector_size=VECTOR_SIZE,\n",
        "    window=WINDOW,\n",
        "    min_count=MIN_COUNT,\n",
        "    workers=WORKERS,\n",
        "    sg=SG,\n",
        "    min_n=MIN_N,\n",
        "    max_n=MAX_N,\n",
        "    alpha=ALPHA,\n",
        "    min_alpha=MIN_ALPHA\n",
        ")\n",
        "\n",
        "print(\"Building vocabulary...\")\n",
        "fasttext_model.build_vocab(corpus_iterator(X_train))\n",
        "print(f\"Vocabulary size: {len(fasttext_model.wv)}\")\n",
        "\n",
        "print(f\"Training FastText model for {EPOCHS} epochs...\")\n",
        "fasttext_model.train(\n",
        "    corpus_iterator(X_train),\n",
        "    total_examples=fasttext_model.corpus_count,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "print(\"FastText training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f4e8df3",
      "metadata": {
        "id": "7f4e8df3"
      },
      "source": [
        "\\\n",
        "## 7. Model building and training - LSTM <a id='random-forest'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g8ZB8JlfPgsH",
      "metadata": {
        "id": "g8ZB8JlfPgsH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def train_lstm_best(X_train, y_train, X_val, y_val, num_classes=None,\n",
        "                    lstm_units=128, dropout_rate=0.3, epochs=30, batch_size=32,\n",
        "                    name=\"LSTM\"):\n",
        "    \"\"\"\n",
        "    Flexible LSTM trainer for TF-IDF, GloVe, FastText embeddings.\n",
        "\n",
        "    X_train, X_val: np.array (2D for TF-IDF or 3D for embeddings)\n",
        "    y_train, y_val: categorical (one-hot) or integer labels (sparse)\n",
        "    num_classes: detected automatically if None\n",
        "    \"\"\"\n",
        "\n",
        "    # Auto-detect number of classes\n",
        "    if num_classes is None:\n",
        "        if y_train.ndim == 1:  # sparse labels\n",
        "            num_classes = int(np.max(y_train) + 1)\n",
        "        else:  # one-hot labels\n",
        "            num_classes = y_train.shape[1]\n",
        "\n",
        "    input_shape = X_train.shape[1:]\n",
        "    model = Sequential()\n",
        "\n",
        "    # Choose model layers based on input shape\n",
        "    if len(input_shape) == 2:  # 3D embeddings\n",
        "        model.add(Bidirectional(LSTM(lstm_units, return_sequences=False), input_shape=input_shape))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    elif len(input_shape) == 1:  # 2D TF-IDF\n",
        "        model.add(Input(shape=input_shape))\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer + loss selection\n",
        "    if y_train.ndim == 1:\n",
        "        model.add(Dense(num_classes, activation='softmax'))\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    else:\n",
        "        model.add(Dense(num_classes, activation='softmax'))\n",
        "        loss = 'categorical_crossentropy'\n",
        "\n",
        "    model.compile(\n",
        "        loss=loss,\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Early stopping\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    axes[0].plot(history.history['accuracy'], 'b-o', label='Train Accuracy')\n",
        "    axes[0].plot(history.history['val_accuracy'], 'r-s', label='Val Accuracy')\n",
        "    axes[0].set_title(f'{name} Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    axes[1].plot(history.history['loss'], 'b-o', label='Train Loss')\n",
        "    axes[1].plot(history.history['val_loss'], 'r-s', label='Val Loss')\n",
        "    axes[1].set_title(f'{name} Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hcoAT6srdoh8",
      "metadata": {
        "id": "hcoAT6srdoh8"
      },
      "outputs": [],
      "source": [
        "# ===== Train model using TF-IDF features =====\n",
        "\n",
        "model_tfidf, history_tfidf = train_lstm_best(\n",
        "    X_train=X_train_tfidf,\n",
        "    y_train=y_train_enc,\n",
        "    X_val=X_val_tfidf,\n",
        "    y_val=y_val_enc,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    name=\"TF-IDF Classifier\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EhcY6g5EXARq",
      "metadata": {
        "id": "EhcY6g5EXARq"
      },
      "outputs": [],
      "source": [
        "tfidf_model, tfidf_history = tfidf_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bRiHLhhwXaAa",
      "metadata": {
        "id": "bRiHLhhwXaAa"
      },
      "outputs": [],
      "source": [
        "print(type(tfidf_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60nZK_E3g1yb",
      "metadata": {
        "id": "60nZK_E3g1yb"
      },
      "outputs": [],
      "source": [
        "# GloVe LSTM\n",
        "glove_input_shape = X_train_glove_seq.shape[1:]  # (timesteps=1, features)\n",
        "glove_model, glove_history = train_lstm_with_plot(\n",
        "    X_train_glove_seq, y_train_enc,\n",
        "    X_val_glove_seq, y_val_enc,\n",
        "    input_shape=glove_input_shape,\n",
        "    epochs=30,\n",
        "    name=\"GloVe LSTM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cmmPI5aOg4Ch",
      "metadata": {
        "id": "cmmPI5aOg4Ch"
      },
      "outputs": [],
      "source": [
        "# FastText LSTM\n",
        "fasttext_input_shape = X_train_seq.shape[1:]  # (MAX_LEN, vector_size)\n",
        "fasttext_model_lstm, fasttext_history = train_lstm_with_plot(\n",
        "    X_train_seq, y_train_enc,\n",
        "    X_val_seq, y_val_enc,\n",
        "    input_shape=fasttext_input_shape,\n",
        "    epochs=30\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z2HPgjxcfUAX",
      "metadata": {
        "id": "z2HPgjxcfUAX"
      },
      "source": [
        "## 8. Results Comparison <a id='comparison'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vfyErUr-Wp6_",
      "metadata": {
        "id": "vfyErUr-Wp6_"
      },
      "outputs": [],
      "source": [
        "print(\"TF-IDF model:\", type(tfidf_model))\n",
        "print(\"GloVe model:\", type(glove_model))\n",
        "print(\"FastText model:\", type(fasttext_model_lstm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3FYg_JuAdqVf",
      "metadata": {
        "id": "3FYg_JuAdqVf"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    report = classification_report(y_test, y_pred, target_names=CATEGORIES)\n",
        "    return report\n",
        "\n",
        "print(\"TF-IDF LSTM Test Metrics:\\n\")\n",
        "print(evaluate_model(tfidf_model, X_test_tfidf_seq, y_test_enc))\n",
        "\n",
        "print(\"\\nGloVe LSTM Test Metrics:\\n\")\n",
        "print(evaluate_model(glove_model, X_test_glove_seq, y_test_enc))\n",
        "\n",
        "print(\"\\nFastText LSTM Test Metrics:\\n\")\n",
        "print(evaluate_model(fasttext_model_lstm, X_test_seq, y_test_enc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NkHkJLJueIX1",
      "metadata": {
        "id": "NkHkJLJueIX1"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_with_cm(model, X_test, y_test, categories, name=\"Model\"):\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "    print(f\"\\n{name} Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=categories))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=categories, yticklabels=categories, cmap='Blues')\n",
        "    plt.title(f'{name} Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    # Return metrics\n",
        "    return {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, average='macro'),\n",
        "        'Recall': recall_score(y_test, y_pred, average='macro'),\n",
        "        'F1': f1_score(y_test, y_pred, average='macro')\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JYCsbyDudsKn",
      "metadata": {
        "id": "JYCsbyDudsKn"
      },
      "outputs": [],
      "source": [
        "def summarize_metrics(model, X_test, y_test, name):\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "    return {\n",
        "        'Embedding': name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, average='macro'),\n",
        "        'Recall': recall_score(y_test, y_pred, average='macro'),\n",
        "        'F1': f1_score(y_test, y_pred, average='macro')\n",
        "    }\n",
        "\n",
        "results = pd.DataFrame([\n",
        "    summarize_metrics(tfidf_model, X_test_tfidf_seq, y_test_enc, 'TF-IDF'),\n",
        "    summarize_metrics(glove_model, X_test_glove_seq, y_test_enc, 'GloVe'),\n",
        "    summarize_metrics(fasttext_model_lstm, X_test_seq, y_test_enc, 'FastText')\n",
        "])\n",
        "\n",
        "print(\"\\nComparison of LSTM Models with Different Embeddings:\")\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f0620bb",
      "metadata": {
        "id": "0f0620bb"
      },
      "source": [
        "\n",
        "\n",
        "TF-IDF is the best\n",
        "\n",
        "Highest accuracy (96.6%) and F1 (96.4%).\n",
        "\n",
        "Dense bag-of-words features seem to capture category-specific words very well.\n",
        "\n",
        "Makes sense because your dataset is relatively small (~1335 training samples) — TF-IDF is very effective in low-data settings.\n",
        "\n",
        "GloVe is very close\n",
        "\n",
        "Accuracy 96.2%, F1 96.0%.\n",
        "\n",
        "Pretrained embeddings work well, but since you averaged word vectors for the entire document (no real sequence), some subtle contextual info is lost.\n",
        "\n",
        "Still slightly worse than TF-IDF here, likely because averaging smooths out important discriminative words.\n",
        "\n",
        "FastText is slightly lower\n",
        "\n",
        "Accuracy 94.6%, F1 94.5%.\n",
        "\n",
        "Works reasonably well, but the gap shows that subword information wasn’t enough to beat TF-IDF.\n",
        "\n",
        "Could be improved if you trained the LSTM on full word sequences instead of document-level averages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ed96408",
      "metadata": {
        "id": "0ed96408"
      },
      "source": [
        "**bold text**\n",
        "##9. Conclusion <a id='conclusion'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e6e45e",
      "metadata": {
        "id": "74e6e45e"
      },
      "source": [
        "###Embeddings Comparison:\n",
        "\n",
        "**TF-IDF**: Performed the best on this dataset, achieving 96.6% accuracy. Captures important words in each category effectively, especially for smaller datasets.\n",
        "\n",
        "**GloVe**: Slightly lower performance (96.2% accuracy), but pretrained embeddings capture semantic relationships between words. Averaging word vectors works well, but some context is lost.\n",
        "\n",
        "**FastText**: Accuracy of 94.6%, slightly behind the others. Subword information is robust for rare words, but averaging across the document reduces its advantage.\n",
        "\n",
        "###Model Comparison:\n",
        "\n",
        "**LSTM with TF-IDF**: Works surprisingly well on small-to-medium datasets, leveraging key words in the sequence.\n",
        "\n",
        "**LSTM with GloVe**: Slightly lower than TF-IDF due to averaging embeddings, but strong generalization potential.\n",
        "\n",
        "**LSTM with FastText**: Decent performance, could improve with sequence-level input rather than averaged embeddings.\n",
        "\n",
        "###Next Steps:\n",
        "\n",
        "Perform hyperparameter tuning (learning rate, LSTM units, dropout) to improve performance.\n",
        "\n",
        "Explore ensemble methods combining multiple embeddings (e.g., TF-IDF + GloVe) to capture both statistical and semantic features.\n",
        "\n",
        "Conduct error analysis on misclassified examples to understand model weaknesses.\n",
        "\n",
        "Compare results with other deep learning models (RNN, GRU) to see if different architectures improve performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}